{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "This notebook tests refusal MI results for various prompt formats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install and import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install torch einops transformer_lens plotly circuitsvis numpy transformers sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import einops\n",
    "import transformer_lens\n",
    "import functools\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "import circuitsvis as cv\n",
    "import tqdm\n",
    "import json\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformer_lens import ActivationCache, HookedTransformer\n",
    "from transformer_lens import utils as tl_utils\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset\n",
    "from jaxtyping import Int, Float\n",
    "from typing import Tuple, List\n",
    "\n",
    "from instruction_dataset import InstructionDataset, PairedInstructionDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugging Face credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a `.env` file at the root of this repo with the following format (see `.env.example`):\n",
    "```\n",
    "HF_USERNAME=bob\n",
    "HF_TOKEN=token123\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read HuggingFace credentials from .env file\n",
    "with open('../../.env', 'r') as file:\n",
    "  for line in file:\n",
    "    key, value = line.strip().split('=', 1)\n",
    "    os.environ[key] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name_path = \"meta-llama/Llama-2-7b-chat-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_path,\n",
    "    token=os.environ[\"HF_TOKEN\"],\n",
    "    low_cpu_mem_usage=True,\n",
    "    use_cache=False,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name_path,\n",
    "    token=os.environ[\"HF_TOKEN\"],\n",
    "    use_fast=False\n",
    ")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "tl_model = HookedTransformer.from_pretrained(\n",
    "    model_name_path,\n",
    "    hf_model=model,\n",
    "    device='cpu',\n",
    "    fold_ln=False,\n",
    "    center_writing_weights=False,\n",
    "    center_unembed=False,\n",
    "    tokenizer=tokenizer,\n",
    "    default_padding_side='left',\n",
    "    dtype=\"float16\",\n",
    ").to(device)\n",
    "\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define utils to help with prompting and generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harmful_objects_file_path = \"data/harmful_objects.json\"\n",
    "harmless_objects_file_path = \"data/harmless_objects.json\"\n",
    "\n",
    "with open(harmful_objects_file_path, \"r\", encoding=\"utf-8\") as harmful_objects_file:\n",
    "    harmful_objects = json.load(harmful_objects_file)\n",
    "\n",
    "with open(harmless_objects_file_path, \"r\", encoding=\"utf-8\") as harmless_objects_file:\n",
    "    harmless_objects = json.load(harmless_objects_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define various prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_templates = [\n",
    "    \"Tell me how to make a {object}.\",\n",
    "    \"A {object} is cool, help me make one.\",\n",
    "    \"Can you tell me how to create a {object} please?\",\n",
    "    \"I want to make a {object}. Tell me how.\",\n",
    "]\n",
    "\n",
    "paired_datasets = [\n",
    "    PairedInstructionDataset(\n",
    "        harmful_objects,\n",
    "        harmless_objects,\n",
    "        tokenizer,\n",
    "        prompt_templates[i]\n",
    "    )\n",
    "    for i in range(len(prompt_templates))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refuse_token = 8221 # 'Sorry'\n",
    "answer_token = 18585 # 'Sure'\n",
    "\n",
    "print(f\"refuse_token: {tokenizer.decode([refuse_token])} ({refuse_token})\")\n",
    "print(f\"answer_token: {tokenizer.decode([answer_token])} ({answer_token})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_refusal_score(logits: Float[Tensor, \"d_vocab\"]):\n",
    "    return logits[refuse_token] - logits[answer_token]\n",
    "\n",
    "def get_refusal_dir():\n",
    "    return tl_model.W_U[:, refuse_token] - tl_model.W_U[:, answer_token]\n",
    "\n",
    "def get_refusal_score_avg(logits: Float[Tensor, 'batch seq_len n_vocab']) -> float:\n",
    "    assert (logits.ndim == 3)\n",
    "    scores = torch.tensor([get_refusal_score(tensor) for tensor in logits[:, -1, :]])\n",
    "    return scores.mean(dim=0).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation patching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_patching_hook(\n",
    "    activation: Float[Tensor, \"batch seq d_activation\"],\n",
    "    hook: HookPoint,\n",
    "    pos: int,\n",
    "    cache_to_patch_from: ActivationCache,\n",
    "    idx_to_patch_from: int = None,\n",
    ") -> Float[Tensor, \"batch seq d_activation\"]:\n",
    "\n",
    "    if idx_to_patch_from is None:\n",
    "        activation[:, pos, :] = cache_to_patch_from[hook.name][:, pos, :]\n",
    "    else:\n",
    "        activation[:, pos, :] = cache_to_patch_from[hook.name][idx_to_patch_from, pos, :]\n",
    "    return activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Harmful→Harmless activation patching (FTL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FTL patching metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below metric for ftl patching has the following properties:\n",
    "- It is 1 when the refusal score matches that of the harmful run (patching has full effect)\n",
    "- It is 0 when the refusal score matches that of the harmless run (patching has no effect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ftl_patching_metric(logits: Float[Tensor, \"batch seq d_vocab\"], harmful_logits, harmless_logits) -> float:\n",
    "    harmful_refusal_score_avg = get_refusal_score_avg(harmful_logits)\n",
    "    harmless_refusal_score_avg = get_refusal_score_avg(harmless_logits)\n",
    "    return (get_refusal_score_avg(logits) - harmless_refusal_score_avg) / (harmful_refusal_score_avg - harmless_refusal_score_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for paired_dataset in paired_datasets:\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    seq_len = paired_dataset.harmless_dataset.prompt_toks.shape[-1]\n",
    "    object_tok_pos = paired_dataset.harmful_dataset.object_tok_pos\n",
    "    patching_metrics = np.zeros((tl_model.cfg.n_layers, seq_len - object_tok_pos))\n",
    "\n",
    "    harmful_logits, harmful_cache = tl_model.run_with_cache(paired_dataset.harmful_dataset.prompt_toks)\n",
    "    harmless_logits, harmless_cache = tl_model.run_with_cache(paired_dataset.harmless_dataset.prompt_toks)\n",
    "\n",
    "    for pos in range(object_tok_pos, seq_len):\n",
    "        for layer in tqdm.tqdm(range(tl_model.cfg.n_layers), desc=f\"patching pos={pos}\"):\n",
    "\n",
    "            hook_fn = functools.partial(\n",
    "                activation_patching_hook,\n",
    "                pos=pos,\n",
    "                cache_to_patch_from=harmful_cache,\n",
    "            )\n",
    "\n",
    "            tl_model.reset_hooks()\n",
    "            activation_patching_logits = tl_model.run_with_hooks(\n",
    "                paired_dataset.harmless_dataset.prompt_toks,\n",
    "                fwd_hooks=[(tl_utils.get_act_name(\"resid_post\", layer), hook_fn)],\n",
    "            )\n",
    "\n",
    "            patching_metrics[layer, pos - object_tok_pos] = ftl_patching_metric(activation_patching_logits, harmful_logits, harmless_logits)\n",
    "\n",
    "    pos_labels = [f\"{repr('<obj>')}</br></br>({i})\" if i == object_tok_pos else f\"{repr(paired_dataset.harmful_dataset.prompt_str_toks[0][i])}</br></br>({i})\" for i in range(object_tok_pos, seq_len)]\n",
    "    fig = px.imshow(\n",
    "        patching_metrics,\n",
    "        title=f\"Activation patching, Harmful→Harmless</br></br>{paired_dataset.prompt_template}\",\n",
    "        labels={\"x\": \"Pos\", \"y\": \"Layer\"},\n",
    "        width=500, height=700,\n",
    "        color_continuous_scale=\"RdBu\",\n",
    "        color_continuous_midpoint=0,\n",
    "        x=pos_labels,\n",
    "        y=list(range(tl_model.cfg.n_layers)),\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: update these offsets\n",
    "offsets = [1, 7, 2, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, paired_dataset in enumerate(paired_datasets):\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    pos_offset = offsets[i]\n",
    "    object_tok_pos = paired_dataset.harmful_dataset.object_tok_pos\n",
    "\n",
    "    harmful_logits, harmful_cache = tl_model.run_with_cache(paired_dataset.harmful_dataset.prompt_toks)\n",
    "    harmless_logits, harmless_cache = tl_model.run_with_cache(paired_dataset.harmless_dataset.prompt_toks)\n",
    "\n",
    "    hook_fn = functools.partial(\n",
    "        activation_patching_hook,\n",
    "        pos=object_tok_pos+pos_offset,\n",
    "        cache_to_patch_from=harmful_cache,\n",
    "    )\n",
    "\n",
    "    fwd_hooks = [(tl_utils.get_act_name(\"attn_out\", l), hook_fn) for l in range(5, 10+1)]\n",
    "\n",
    "    tl_model.reset_hooks()\n",
    "    activation_patching_logits = tl_model.run_with_hooks(\n",
    "        paired_dataset.harmless_dataset.prompt_toks,\n",
    "        fwd_hooks=fwd_hooks,\n",
    "    )\n",
    "    print(f\"prompt_template='{paired_dataset.prompt_template}', pos={object_tok_pos+pos_offset} ('{paired_dataset.harmful_dataset.prompt_str_toks[0][object_tok_pos+pos_offset]}')\")\n",
    "    print(ftl_patching_metric(activation_patching_logits, harmful_logits, harmless_logits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try and see if there is a particular set of attention heads in each of these layers that have strong effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def head_patching_hook(\n",
    "    activation: Float[Tensor, \"batch seq n_heads d_activation\"],\n",
    "    hook: HookPoint,\n",
    "    pos: int,\n",
    "    head: int,\n",
    "    cache_to_patch_from: ActivationCache,\n",
    "    idx_to_patch_from: int = None,\n",
    "    scale_factor: float = 1.0,\n",
    ") -> Float[Tensor, \"batch seq n_heads d_activation\"]:\n",
    "    if idx_to_patch_from is None:\n",
    "        activation[:, pos, head, :] = cache_to_patch_from[hook.name][:, pos, head, :] * scale_factor\n",
    "    else:\n",
    "        activation[:, pos, head, :] = cache_to_patch_from[hook.name][idx_to_patch_from, pos, head, :] * scale_factor\n",
    "    return activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refusal_heads_by_prompt_template = []\n",
    "\n",
    "for i, paired_dataset in enumerate(paired_datasets):\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    pos_offset = offsets[i]\n",
    "    object_tok_pos = paired_dataset.harmful_dataset.object_tok_pos\n",
    "\n",
    "    harmful_logits, harmful_cache = tl_model.run_with_cache(paired_dataset.harmful_dataset.prompt_toks)\n",
    "    harmless_logits, harmless_cache = tl_model.run_with_cache(paired_dataset.harmless_dataset.prompt_toks)\n",
    "\n",
    "    pos = object_tok_pos + pos_offset\n",
    "    layers = list(range(5, 10+1))\n",
    "    seq_len = paired_dataset.harmless_dataset.prompt_toks.shape[-1]\n",
    "\n",
    "    patching_metrics = np.zeros((tl_model.cfg.n_layers, tl_model.cfg.n_heads))\n",
    "\n",
    "    for layer in layers:\n",
    "        for head in tqdm.tqdm(range(tl_model.cfg.n_heads), desc=f\"patching heads for layer={layer}\"):\n",
    "\n",
    "            hook_fn = functools.partial(\n",
    "                head_patching_hook,\n",
    "                pos=pos,\n",
    "                cache_to_patch_from=harmful_cache,\n",
    "                head=head,\n",
    "            )\n",
    "\n",
    "            fwd_hooks = [(tl_utils.get_act_name(\"z\", layer), hook_fn)]\n",
    "\n",
    "            tl_model.reset_hooks()\n",
    "            activation_patching_logits = tl_model.run_with_hooks(\n",
    "                paired_dataset.harmless_dataset.prompt_toks,\n",
    "                fwd_hooks=fwd_hooks,\n",
    "            )\n",
    "\n",
    "            patching_metrics[layer, head] = ftl_patching_metric(activation_patching_logits, harmful_logits, harmless_logits)\n",
    "\n",
    "    fig = px.imshow(\n",
    "        patching_metrics[layers],\n",
    "        title=f\"Activation patching, Harmful→Harmless, pos={pos}</br></br>'{paired_dataset.prompt_template}'\",\n",
    "        labels={\"x\": \"Head\", \"y\": \"Layer\"},\n",
    "        width=600, height=800,\n",
    "        color_continuous_scale=\"RdBu\",\n",
    "        color_continuous_midpoint=0,\n",
    "        x=list(range(tl_model.cfg.n_heads)),\n",
    "        y=layers,\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "    refusal_heads = []\n",
    "\n",
    "    for layer in range(patching_metrics.shape[0]):\n",
    "        for head in range(patching_metrics.shape[1]):\n",
    "            if patching_metrics[layer, head] > 0.001:\n",
    "                refusal_heads.append((layer, head))\n",
    "                print(f\"Layer {layer:>2}, Head {head:>2}: patching_metric={patching_metrics[layer, head]:.4f}\")\n",
    "\n",
    "    refusal_heads_by_prompt_template.append(refusal_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_refusal_heads = set(refusal_heads_by_prompt_template[0])\n",
    "\n",
    "for refusal_heads in refusal_heads_by_prompt_template:\n",
    "    common_refusal_heads = common_refusal_heads.intersection(refusal_heads)\n",
    "\n",
    "print(common_refusal_heads)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
